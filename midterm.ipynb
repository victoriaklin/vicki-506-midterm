{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "import gc\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_ids = pd.read_csv('test.csv')\n",
    "\n",
    "test_data = train_data.merge(test_ids['Id'], on=\"Id\")\n",
    "train_data = train_data.dropna(subset=\"Score\")\n",
    "\n",
    "train_data = train_data.sample(frac=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039739, 9)\n",
      "(212192, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: preprocessing by filling in blank values and converting values to proper format\n",
    "\n",
    "def preprocess_1(data):\n",
    "    data['HelpfulnessRatio'] = np.where(data['HelpfulnessDenominator'] > 0,\n",
    "                                                   data['HelpfulnessNumerator'] / data['HelpfulnessDenominator'], \n",
    "                                                   0)\n",
    "    \n",
    "    data['LogHelpfulnessNumerator'] = np.log1p(data['HelpfulnessNumerator'])\n",
    "    data['LogHelpfulnessDenominator'] = np.log1p(data['HelpfulnessDenominator'])\n",
    "    \n",
    "    # Convert 'Time' to datetime and extract year and month if 'Time' column exists\n",
    "    if 'Time' in data.columns:\n",
    "        data['Time'] = pd.to_datetime(data['Time'], unit='s')\n",
    "        data['Year'] = data['Time'].dt.year\n",
    "        data['Month'] = data['Time'].dt.month\n",
    "    else:\n",
    "        # If 'Time' column is missing, fill with default values\n",
    "        data['Year'] = 0\n",
    "        data['Month'] = 0\n",
    "    return data\n",
    "\n",
    "train_data = preprocess_1(train_data)\n",
    "test_data = preprocess_1(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 11 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d7d6e9e4394ceda3363aa3c4cec504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=94522), Label(value='0 / 94522')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18436) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10093cca1d964d46b0c497a4de8b8e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=94522), Label(value='0 / 94522')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18437) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18438) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18439) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18440) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18441) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18442) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18443) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18444) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18445) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18446) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18447) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18513) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09286c6a37f9419c886b9083bc41eba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=19291), Label(value='0 / 19291')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18514) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18515) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18516) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18517) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18518) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18519) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18520) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18521) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18522) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18523) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18524) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18571) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfbec61d4e754544b4f6229994f4fd19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=19291), Label(value='0 / 19291')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18572) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18573) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18574) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18575) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18576) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18577) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18578) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18579) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18580) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18581) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18582) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel \n",
    "\n",
    "def tokenize(x):\n",
    "    return word_tokenize(str(x).lower())\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "train_data['TextTokens'] = train_data['Text'].parallel_apply(tokenize)\n",
    "train_data['SummaryTokens'] = train_data['Summary'].parallel_apply(tokenize)\n",
    "test_data['TextTokens'] = test_data['Text'].parallel_apply(tokenize)\n",
    "test_data['SummaryTokens'] = test_data['Summary'].parallel_apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: sentiment analysis by positive and negative words\n",
    "\n",
    "def preprocess_2(data):\n",
    "    # these words are some words that can reasonably be expected to be found in positive and negative reviews\n",
    "    positive_words = set([\n",
    "        \"amazing\", \"awesome\", \"best\", \"brilliant\", \"captivating\", \"charming\", \"compelling\", \n",
    "        \"delightful\", \"enjoyable\", \"entertaining\", \"excellent\", \"fantastic\", \"fascinating\", \n",
    "        \"fun\", \"great\", \"impressive\", \"incredible\", \"masterpiece\", \"memorable\", \"outstanding\", \n",
    "        \"perfect\", \"remarkable\", \"spectacular\", \"stunning\", \"superb\", \"thrilling\", \"touching\", \n",
    "        \"unforgettable\", \"wonderful\", \"worthy\"\n",
    "    ])\n",
    "    \n",
    "    negative_words = set([\n",
    "        \"awful\", \"boring\", \"cliched\", \"disappointing\", \"dreadful\", \"dull\", \"flat\", \"forgettable\", \n",
    "        \"horrible\", \"inconsistent\", \"lame\", \"mediocre\", \"messy\", \"nonsensical\", \"poor\", \"predictable\", \n",
    "        \"ridiculous\", \"shallow\", \"stale\", \"terrible\", \"unconvincing\", \"uninteresting\", \"uninspired\", \n",
    "        \"weak\", \"worthless\", \"worse\", \"worst\"\n",
    "    ])\n",
    "\n",
    "    neutral_words = set([\n",
    "        \"mediocre\", \"average\", \"okay\", \"passable\", \"acceptable\", \"fine\", \"decent\", \"typical\",\n",
    "        \"standard\", \"ordinary\", \"satisfactory\", \"modest\", \"usual\", \"unremarkable\", \"run-of-the-mill\",\n",
    "        \"expected\", \"serviceable\", \"all right\", \"not bad\", \"workable\", \"sufficient\"\n",
    "    ])\n",
    "    \n",
    "    def simple_sentiment_analysis(text):\n",
    "        \"\"\" takes text and checks it for positive and negative words (connotations) \"\"\"\n",
    "        pos, neg, neu = 0, 0, 0\n",
    "        \n",
    "        for word in text:\n",
    "            if word in positive_words:\n",
    "                pos += 1\n",
    "            if word in negative_words:\n",
    "                neg += 1\n",
    "            if word in neutral_words:\n",
    "                neu += 1\n",
    "            \n",
    "        return pos, neg, neu, pos-neg  # Positive values are more positive, negative values more negative\n",
    "\n",
    "    # Apply sentiment analysis to Text and Summary\n",
    "    data['PositiveWords'], data['NegativeWords'], data['NeutralWords'], data['SentimentText'] = zip(*data['TextTokens'].apply(simple_sentiment_analysis))\n",
    "    _, _, _, data['SentimentSummary'] = zip(*data['SummaryTokens'].apply(simple_sentiment_analysis))\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = preprocess_2(train_data)\n",
    "test_data = preprocess_2(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Feature Engineering - Text Preprocessing\n",
    "\n",
    "def preprocess_3(data):\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'[^a-zA-Z!.?\\s]', '', str(text))\n",
    "        text = text.lower()\n",
    "        return text\n",
    "    \n",
    "    # Apply text cleaning to training data\n",
    "    data['CleanText'] = data['Text'].apply(clean_text)\n",
    "    data['CleanSummary'] = data['Summary'].apply(clean_text)\n",
    "    \n",
    "    # this line adds a new feature that is the length of the text\n",
    "    data['TextLen'] = data['CleanText'].apply(lambda x : len(x))\n",
    "    data['AvgWordLen'] = data['TextTokens'].apply(lambda x : np.mean([len(word) for word in x]))\n",
    "    \n",
    "    # this line adds a new feature that is the number of exclamation marks in the text\n",
    "    data['NumExclamations'] = data['CleanText'].apply(lambda x : x.count('!'))\n",
    "    data['NumQuestions'] = data['CleanText'].apply(lambda x : x.count('?'))\n",
    "\n",
    "    return data\n",
    "\n",
    "train_data = preprocess_3(train_data)\n",
    "test_data = preprocess_3(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded glove\n",
      "done with train set\n",
      "done with test set\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(file_path, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Create an embedding for each document by averaging word embeddings\n",
    "def get_document_embedding(text, embeddings_index, embedding_dim=100):\n",
    "    word_vectors = [embeddings_index[word] for word in text if word in embeddings_index]\n",
    "    if word_vectors:\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)\n",
    "\n",
    "# Load the GloVe embeddings (adjust file path and dimension as needed)\n",
    "glove_path = '/Users/victorialin/Downloads/glove/glove.6B.300d.txt'  # Update this to your file path\n",
    "embedding_dim = 300  # Adjust based on the GloVe file used\n",
    "embeddings_index = load_glove_embeddings(glove_path, embedding_dim=embedding_dim)\n",
    "\n",
    "print(\"Loaded glove\")\n",
    "\n",
    "text_vectors_train = np.array([get_document_embedding(review, embeddings_index, embedding_dim) for review in train_data['TextTokens']])\n",
    "\n",
    "summary_vectors_train = np.array([get_document_embedding(review, embeddings_index, embedding_dim) for review in train_data['SummaryTokens']])\n",
    "\n",
    "print(\"done with train set\")\n",
    "\n",
    "text_vectors_test = np.array([get_document_embedding(review, embeddings_index, embedding_dim) for review in test_data['TextTokens']])\n",
    "summary_vectors_test = np.array([get_document_embedding(review, embeddings_index, embedding_dim) for review in test_data['SummaryTokens']])\n",
    "\n",
    "print(\"done with test set\")\n",
    "\n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert ProductIds to one-hot, do PCA\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "encoder = HashingVectorizer()\n",
    "encoded_ids = encoder.fit_transform(train_data['ProductId'])\n",
    "encoded_ids_test = encoder.transform(test_data['ProductId'])\n",
    "\n",
    "pca = PCA(n_components=20)  # Adjust n_components based on desired dimensionality\n",
    "reduced_ids = pca.fit_transform(encoded_ids)\n",
    "reduced_ids_test = pca.transform(encoded_ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(212192, 20)\n"
     ]
    }
   ],
   "source": [
    "print(reduced_ids_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying sentiment analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18686) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e912b42b0a4febaa33f94876dfe0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=94522), Label(value='0 / 94522')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18687) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18688) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18689) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18690) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18691) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18692) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18695) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18696) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18697) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18698) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "print(\"Applying sentiment analysis...\")\n",
    "def sentiment_scores(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return pd.Series({\n",
    "        'Sentiment_Neg': scores['neg'],\n",
    "        'Sentiment_Neu': scores['neu'],\n",
    "        'Sentiment_Pos': scores['pos'],\n",
    "        'Sentiment_Compound': scores['compound']\n",
    "    })\n",
    "sentiment_df = train_data['CleanText'].parallel_apply(sentiment_scores)\n",
    "train_data = pd.concat([train_data.reset_index(drop=True), sentiment_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18955) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23564c4cea1409da5f1e6614eec6be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=19291), Label(value='0 / 19291')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(18956) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18957) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18958) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18959) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18960) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18961) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18962) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18963) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18964) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18965) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(18966) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    }
   ],
   "source": [
    "sentiment_test = test_data['CleanText'].parallel_apply(sentiment_scores)\n",
    "test_data = pd.concat([test_data.reset_index(drop=True), sentiment_test.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize numeric features\n",
    "feats = [\n",
    "    'HelpfulnessRatio', 'LogHelpfulnessNumerator', 'LogHelpfulnessDenominator', 'SentimentText', 'SentimentSummary', \n",
    "    'Year', 'Month', 'TextLen', 'NumExclamations', 'NumQuestions', 'PositiveWords', 'NegativeWords', 'AvgWordLen', 'NeutralWords',\n",
    "    'Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound'\n",
    "]\n",
    "\n",
    "numeric_features_train = train_data[feats]\n",
    "numeric_features_test = test_data[feats]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_features_scaled = scaler.fit_transform(numeric_features_train)\n",
    "numeric_features_scaled_test = scaler.transform(numeric_features_test)\n",
    "\n",
    "\n",
    "# this is the FULL DATASET THAT IS FULLY CLEANED\n",
    "X = np.hstack([numeric_features_scaled, text_vectors_train, summary_vectors_train, reduced_ids])\n",
    "X_test = np.hstack([numeric_features_scaled_test, text_vectors_test, summary_vectors_test, reduced_ids_test])\n",
    "y = train_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1039739, 638)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: data splitting\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # shift y values for XGBoost\n",
    "y_train_shifted = [int(x-1) for x in y_train]\n",
    "y_val_shifted = [int(x-1) for x in y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(19048) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19049) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19050) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19051) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19052) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19053) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19054) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19055) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19056) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19057) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19058) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(19060) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6397224306076519\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.63      0.62     12751\n",
      "           1       0.42      0.23      0.30     12468\n",
      "           2       0.43      0.34      0.38     24597\n",
      "           3       0.47      0.29      0.36     46920\n",
      "           4       0.72      0.90      0.80    111212\n",
      "\n",
      "    accuracy                           0.64    207948\n",
      "   macro avg       0.53      0.48      0.49    207948\n",
      "weighted avg       0.60      0.64      0.61    207948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', max_iter=1000, n_jobs=-1)\n",
    "logreg.fit(X_train, y_train_shifted)\n",
    "\n",
    "y_pred = logreg.predict(X_val)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_val_shifted, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val_shifted, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file generated!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 8: Generate Predictions for the Test Set\n",
    "\n",
    "# predict\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "\n",
    "# shift back to class labels\n",
    "y_test_pred = y_test_pred + 1\n",
    "\n",
    "# Step 9: Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_data['Id'],\n",
    "    'Score': y_test_pred\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file generated!\")\n",
    "\n",
    "# Optional: Clear memory to avoid kernel crashes\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
